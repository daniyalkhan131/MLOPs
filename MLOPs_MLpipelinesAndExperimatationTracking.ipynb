{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlC1JJ9_Wls4"
      },
      "outputs": [],
      "source": [
        "#in terminal we need to track before commit using git add but in IDE like vscode traacking is done automattically\n",
        "#while in dvc tracking is done automatically in both so need to do add"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data pipeline --> experimentaton pipeline --> eployment and monitoring pipeline\n",
        "#and this is ML e2e pipeline\n",
        "\n",
        "#but experimentation pipeline have loops so it is DAG, we can call a pipline a DAG"
      ],
      "metadata": {
        "id": "F2hKAnkVXH6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learn about enviorments in anaconda and python and computer........all..........."
      ],
      "metadata": {
        "id": "JDodXyGcZv2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##projects steps\n",
        "\n",
        "#git init\n",
        "#dvc init\n",
        "#reate data folder\n",
        "#dvc add data/\n",
        "#git add data.dvc .gitignore\n",
        "#add test python script\n",
        "#make requirments.txt for installing libraries needed for this\n",
        "\n",
        "#use dvclive to track something, logging the things, using this we track for every epoch , in experiments we can see that\n",
        "#we can change params.yaml file in dvclive to control things, anf for manipulating .yaml use yaml lib\n",
        "#in experimental area we can modeify params from params.yaml file, we dont need to change things in code, we changes\n",
        "#params and automatically things get version control and net output of experiment comes\n",
        "#like when test train split percentage change then data change as train test different so doing experiments by\n",
        "#changing these and all things are tracked, people in insdustry change seed value, percentage so base data change\n",
        "#and they dont track that, and things got overwrite\n",
        "#we can visualise experiments we did\n",
        "\n",
        "#we use live.log_params for paramter setting\n",
        "#and for dataset selection criteria using params use live.log_metric\n",
        "#by using these we are tracking experiments\n",
        "\n",
        "#in the data pipeline we are manipulating data using .py code like preprocessing data, then convertiing to features.csv\n",
        "#then to train val and these things are tracked by dvc\n",
        "#then code and data created now experimentation part come where we change params without modefying code\n",
        "#and tracking the experiments result\n",
        "#and we define stages in dvc.yaml, command we define that make these three pipeline run one after another\n",
        "\n",
        "#if we change the params.yaml and then do dvc repro means reproduce the result so if chnage test train split then\n",
        "#from stage of data pipeline things execute and if no changes are made then dvc shows data and pipeline uptodate\n",
        "#this pipeline message is for this pipeline we create\n",
        "#this is the way experimentatoins are done with tracking using dvc\n",
        "\n",
        "#we define stages or flow of execution of python files like make_dataset train etc in dvc.yaml file and in that\n",
        "#we dine command,python file to call, data, output of that data\n",
        "\n",
        "#pipeline we studies in ML was very simple one and here the same pipeline is there in which things are executing one\n",
        "#after another but in a version controlled manner or trackking manner\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FQybnXCLe0OV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#need to properly setup vscode with github"
      ],
      "metadata": {
        "id": "-dgkKXfF9hVR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#when we get an experiment whose perfornce comes best so we create branch means push that to github, and in github\n",
        "#we have full info regarding this means what data created, preprocessed,changes and all tracked\n",
        "#then after this CI/CD comes using that we create setup in that this full things will run in integration on an\n",
        "#isolated server using docker or directly , docker is best practice, we create docker of that branch that is best\n"
      ],
      "metadata": {
        "id": "fZxtWJZYkPyV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#every mature company use mlflow\n",
        "#it is build for experimentation pipeline but have capability for deployment&monitoring pipeline\n",
        "#we are not creating pipeline with mlflow only tracking experimental pipeline with this, in industry pipeline creation is\n",
        "#done using cubeflow with mlflow\n",
        "#but in this we do create pipel with dvc\n",
        "\n",
        "#DVC build for data pipeline but have capability for experimentation pipeline\n",
        "\n",
        "#for experimentation pipeline only chnage live with mlflow and remaining code is same and experiments we get to\n",
        "#see on mlflow server in cmd write\n",
        "\n"
      ],
      "metadata": {
        "id": "cT9pXy1tnr1H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#there is a way of writing pipeline in any tool and in dvc it is called stages, in cubeflow(build on kubernates)\n",
        "# it is called something else but thin core concept remain same"
      ],
      "metadata": {
        "id": "zLIRe83RoIM6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXrAbKdoyUT_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}